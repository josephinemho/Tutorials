{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "1. Clean and transform data\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Handle imbalanced classes\n",
    "4. **Modeling & evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We shouldn't be using classification accuracy/error rate for evaluating classifiers, due to huge class imbalance in our dataset. Accuracy applies a 0.50 threahold to decide between classes, which is not our case.\n",
    "\n",
    "**We want probabilities of class memberships instead of just labels.**\n",
    "\n",
    "Instead, precision-recall curves predict probabilities of an observation belonging to each class in a classification problem rather than predicting the classes directly. We will be using precision-recall (instead of ROC curves) because we are dealing with class imbalance and precision-recall calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, because we are generally less interested in the ability of the model predicting class 0 correctly (which we have a lot of).\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{True&space;Positives}{(True&space;Positives&space;&plus;&space;False&space;Positives)&space;}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{True&space;Positives}{(True&space;Positives&space;&plus;&space;False&space;Positives)&space;}\" title=\"\\frac{True Positives}{(True Positives + False Positives) }\" /></a>\n",
    "\n",
    "The calculations do not make use of the true negatives. It is only concerned with the correct prediction of the miniority class (`failure=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load('../work/data/X_train')\n",
    "y_train = joblib.load('../work/data/y_train')\n",
    "X_train_resampled = joblib.load('../work/data/X_train_resampled')\n",
    "y_train_resampled = joblib.load('../work/data/y_train_resampled')\n",
    "X_test = joblib.load('../work/data/X_test')\n",
    "y_test = joblib.load('../work/data/y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Draws a line (hyperplane) between different classes of points. The further a point is from the boundary line, the more its score (estimate) increases (nearing 0 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression.get_params(LogisticRegression).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:00:52.155262\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "params = {\n",
    "    'penalty': ['l1','l2'],\n",
    "    'C': np.arange(1,14,3)\n",
    "}\n",
    "\n",
    "lr1 = GridSearchCV(LogisticRegression(), param_grid=params, cv=ShuffleSplit(random_state=88), n_jobs=-1)\n",
    "lr1.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"Time: \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lr1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted failure=0</th>\n",
       "      <th>predicted failure=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true failure=0</th>\n",
       "      <td>33753</td>\n",
       "      <td>3572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true failure=1</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted failure=0  predicted failure=1\n",
       "true failure=0                33753                 3572\n",
       "true failure=1                    5                   19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test, prediction)\n",
    "pd.DataFrame(confusion, columns=['predicted failure=0', 'predicted failure=1'], index=['true failure=0', 'true failure=1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90422769016573401"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our model has a score of 90%, our model over-estimates the likelihood of a failure occurring, even when there is no failure. More importantly, it has some False Negatives, which means we can miss important failures. If the cost of a False Negative is high, we would want to judge our models based on their Recall scores. If the cost of a False Positive is high, we would want to use Precision. \n",
    "\n",
    "However, the instructions say we want to both minimize false positives and false negatives, so we will be using the F1 Score to judge our models, which balances Precision and Recall AND takes into acccount an uneven class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010511756569847857"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:00:46.887099\n"
     ]
    }
   ],
   "source": [
    "# Try it on non-upsampled data\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "params = {\n",
    "    'penalty': ['l1','l2'],\n",
    "    'C': np.arange(1,14,3)\n",
    "}\n",
    "\n",
    "lr2 = GridSearchCV(LogisticRegression(), param_grid=params, cv=ShuffleSplit(random_state=88), n_jobs=-1)\n",
    "lr2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Time: \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999357412514\n",
      "0.0769230769231\n"
     ]
    }
   ],
   "source": [
    "prediction = lr2.predict(X_test)\n",
    "print(lr2.score(X_test, y_test))\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually does better on non-upsampled data. Interesting! Let's see if it's consistent or if it's just a fluke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write classes/functions to repeat on other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Evaluation:\n",
    "    def __init__(self):\n",
    "        self.name = name\n",
    "    \n",
    "    def fit_model(my_model, my_params, X, y):\n",
    "        start = datetime.now()\n",
    "        model = GridSearchCV(my_model(), param_grid=my_params, cv=ShuffleSplit(random_state=88), n_jobs=-1)\n",
    "        model.fit(X, y)\n",
    "        print(\"Time: \", datetime.now() - start)\n",
    "        return model\n",
    "    \n",
    "    def score_model(model, X, y):\n",
    "        prediction = model.predict(X)\n",
    "        print(\"Accuracy Score: \", model.score(X, y))\n",
    "        print(\"F1 Score: \", f1_score(y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Recursively subdivides the instance space into finer and finer subregions until it is all one class (or good enough). New instances start at the root node and takes the appropriate path until it reaches a lead node, which determines the classification by checking the classes of the training instances that reached that leaf, and the majority determines the class. For that leaf, the score is calculated by:\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\frac{majority&space;instances}{(majority&space;instances&space;&plus;&space;minority&space;instances)&space;}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\frac{majority&space;instances}{(majority&space;instances&space;&plus;&space;minority&space;instances)&space;}\" title=\"\\frac{majority instances}{(majority instances + minority instances) }\" /></a>\n",
    "\n",
    "When using scikit-learn's DecisionTreeClassifier, always set min_samples_leaf to something like 5 or 10. Its default value of 1 is useless and is guaranteed to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier.get_params(DecisionTreeClassifier).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': np.arange(1,14,3),\n",
    "    'min_samples_leaf': np.arange(1,14,3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:02:05.669966\n",
      "Accuracy Score:  0.972582933947\n",
      "F1 Score:  0.0077519379845\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree on resampled data\n",
    "\n",
    "dt1 = Model_Evaluation.fit_model(DecisionTreeClassifier, params, X_train_resampled, y_train_resampled)\n",
    "Model_Evaluation.score_model(dt1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:00:45.908535\n",
      "Accuracy Score:  0.999357412514\n",
      "F1 Score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree on non-resampled data\n",
    "\n",
    "dt1 = Model_Evaluation.fit_model(DecisionTreeClassifier, params, X_train, y_train)\n",
    "Model_Evaluation.score_model(dt1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[10,100],\n",
    "    'max_depth':[10,40,None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:08:35.366464\n",
      "Accuracy Score:  0.999009344293\n",
      "F1 Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on resampled data\n",
    "\n",
    "rf1 = Model_Evaluation.fit_model(RandomForestClassifier, params, X_train_resampled, y_train_resampled)\n",
    "Model_Evaluation.score_model(rf1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:02:36.111837\n",
      "Accuracy Score:  0.999357412514\n",
      "F1 Score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on non-resampled data\n",
    "\n",
    "rf2 = Model_Evaluation.fit_model(RandomForestClassifier, params, X_train, y_train)\n",
    "Model_Evaluation.score_model(rf2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor\n",
    "If for example, `k=5`, for every new instance, 5 of its nearest neighbors are randomly selected and some function like majority is applied to the five neighbors. To assign a score, divide the number of positive instances by the total and return the fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_neighbors': np.arange(2,30,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  4:10:37.005268\n",
      "Accuracy Score:  0.997108356315\n",
      "F1 Score:  0.0181818181818\n"
     ]
    }
   ],
   "source": [
    "# KNN on resampled data\n",
    "\n",
    "kn1 = Model_Evaluation.fit_model(KNeighborsClassifier, params, X_train_resampled, y_train_resampled)\n",
    "Model_Evaluation.score_model(kn1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  2:48:13.990020\n",
      "Accuracy Score:  0.999357412514\n",
      "F1 Score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# KNN on resampled data\n",
    "\n",
    "kn2 = Model_Evaluation.fit_model(KNeighborsClassifier, params, X_train, y_train)\n",
    "Model_Evaluation.score_model(kn2, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
